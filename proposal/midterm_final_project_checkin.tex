\documentclass[11pt]{article}

% --- Margins (similar to ACL) ---
\usepackage[a4paper,margin=2.5cm]{geometry}

% --- Fonts ---
\usepackage{times}      % main text font
\usepackage{latexsym}   % symbols for NLP-style papers
\usepackage{url}        % for urls
\usepackage{multirow}

% --- Japanese language support ---
\usepackage[utf8]{inputenc}
\usepackage{CJKutf8}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage[square,numbers]{natbib}  % For bibliography management with square brackets


% --- Section spacing (ACL style tweaks) ---
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{2ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titlespacing*{\subsection}{0pt}{1.8ex plus .5ex minus .2ex}{0.8ex plus .2ex}
\titlespacing*{\subsubsection}{0pt}{1.5ex plus .5ex minus .2ex}{0.5ex plus .2ex}

% --- Caption styling ---
\usepackage{caption}
\captionsetup{font=small, labelfont=bf}

\title{Midterm Final Project Check-in}
\author{
\begin{tabular}{c c c}
YuChen (Jean) Lin & Pakhi Chatterjee & Aditya Pandey\\
\texttt{lin.4842@osu.edu} & \texttt{chatterjee.197@osu.edu} & \texttt{pandey.172@osu.edu}
\end{tabular}
}
\date{}

\begin{document}

\maketitle

\section{Problem Statement}
Toxic comment detection models for Japanese are typically trained and evaluated on native-script text (kanji/kana). In practice, however, social text often appears in rōmaji (romanized Japanese) for ease of typing, stylistic effect, or to evade filters. Yet most open models are not evaluated for \textit{script invariance}: whether predictions stay stable when only the script changes and the underlying Japanese sentence stays the same. \\
\\
If a model flags \begin{CJK}{UTF8}{min}最低だ\end{CJK} as toxic but misses the semantically identical "saiteida," moderation quality and fairness suffer. Recent Japanese toxicity resources exist, but they focus on native script; a paired, cross-script robustness evaluation is missing. We target that gap by building a lightweight script-invariant toxicity pipeline and a paired testbed that measures label stability when only the script changes. 

\section{Motivation}
Japan has tightened penalties for online insults in 2022, making serious cases punishable by up to one year of imprisonment, so reliable toxicity detection in Japanese is no longer just an academic concern. At the same time, moderation quality is known to be uneven outside English, with evidence of under-investment and patchy language coverage across platforms. Romanized Japanese is ubiquitous because standard IMEs accept rōmaji keystrokes and convert them to kana/kanji; users can also intentionally leave text unconverted. \\
\\
If safety systems only work reliably on native script, they will systematically miss toxic content written in rōmaji and treat some speakers less fairly than others. We therefore treat romanized Japanese as a \textit{first-class input type} and ask whether modern models actually behave robustly under script changes.  

\section{Research Gap}
Recent Japanese toxicity datasets such as LLM-jp's Japanese Toxicity Dataset (v2)\cite{llmjp2024toxicity} provide manually labeled toxic comments in native Japanese to support safer LLMs. Industry work on Japanese toxicity detection emphasizes efficiency and deployment concerns but similarly evaluates almost exclusively on native script. Parallel native/romanized benchmarks for Japanese toxicity are, to our knowledge, missing. \\
\\
Romanized text has been recognized as a real failure mode in other languages. For example, Roman Urdu hate-speech detection work constructs dedicated datasets and models because romanization changes surface forms and challenges tokenizers \cite{mathur2020hateval}. A recent Nature-style line of work on multilingual toxicity \cite{nature2024multilingual} highlights script-agnostic robustness as a key requirement for safe moderation, but no equivalent study has focused on Japanese rōmaji. \\
\\
Separately, modern NLP offers heterogeneous modeling approaches:
\begin{itemize}
\item \textbf{Language-specific subword models} such as Japanese BERT \cite{bertjapanesemodel} are strong native-script baselines widely fine-tuned for Japanese tasks.
\item \textbf{Multilingual subword models} such as mDeBERTa-v3 \cite{mdebertav3model} are pre-trained on the CC100 corpus covering ~100 languages, including Japanese, offering a cross-lingual baseline.
\item \textbf{Tokenizer-free models} such as ByT5 \cite{googlebyt5model} operate directly on raw UTF-8 bytes rather than on subword tokens, and are advertised as more robust to spelling noise and script issues.
\end{itemize}
However, we lack a systematic comparison of these model families on a script-controlled Japanese toxicity benchmark. This project fills that gap by building a paired native/rōmaji Japanese toxicity dataset and comparing how a language-specific subword model, a multilingual subword model, and a tokenizer-free byte-level model behave under script changes.

\section{Research Questions and Hypotheses}
We focus on two central research questions:
\begin{itemize}
\item \textbf{Script invariance}: For Japanese toxicity detection, how does model performance change when test inputs are converted from native script to rōmaji while keeping labels fixed?
\item \textbf{Tokenization strategy}: Do tokenizer-free/byte-level models exhibit smaller performance drops and lower “flip rates” under romanization than subword models?
\end{itemize}
Our hypotheses are as follows:
\begin{itemize}
\item \textbf{H1 - Performance Gap:} Tokenized models (BERT Japanese, mDeBERTa) will show larger $|\Delta F_1|$ than ByT5 due to tokenization mismatch on romaji
\item \textbf{H2 - Toxic Recall Preservation:} ByT5 will maintain higher toxic recall when switching to romaji compared to tokenized models
\item \textbf{H3 - Perturbation Resilience:} ByT5 will show smaller F1 degradation under romanization variants than tokenized models
\end{itemize}

\section{Technical Resources and Datasets}
\subsection{Models}
We plan to fine-tune and compare three transformer models that differ primarily in their tokenization strategy:
\begin{itemize}
\item Japanese-specific subword model (Specialist)
\begin{itemize}
\item \textbf{Base model}: {\fontfamily{helvet}\selectfont tohoku-nlp/bert-base-japanese-v3} \cite{bertjapanesemodel}
\item A BERT variant pre-trained specifically on Japanese corpora, providing a strong native-script subword baseline.
\end{itemize}
\item Multilingual subword model (Generalist)
\begin{itemize}
\item \textbf{Base model}: {\fontfamily{helvet}\selectfont microsoft/mdeberta-v3-base} \cite{mdebertav3model}
\item A multilingual DeBERTa-v3 model trained on the CC100 corpus, representing a high-capacity subword model with broad cross-lingual coverage.
\end{itemize}
\item Tokenizer-free byte-level model
\begin{itemize}
\item \textbf{Base model}: {\fontfamily{helvet}\selectfont google/byt5-small} \cite{googlebyt5model}
\item A byte-level T5 variant that operates directly on UTF-8 bytes instead of subword tokens, designed to be robust to spelling and script variation.
\end{itemize}
\end{itemize}
Each model will be equipped with a standard classification head for strict binary toxicity prediction.

\subsection{Training Datasets and Standardized Schema}
We use two public Japanese toxicity datasets:
\begin{itemize}
\item llm-jp-toxicity-dataset-v2 \cite{llmjp2024toxicity}: native Japanese sentences with multi-annotator toxicity labels.
\item inspection-ai/japanese-toxic-dataset \cite{inspection2024japanese}: native Japanese sentences with a three-way toxicity label and category flags.
\end{itemize} 
Both are passed through a Python script which converts them into a single standardized CSV format with:
%{\fontfamily{tgbonum}\selectfont }
\begin{itemize}
\item one row per text ({\fontfamily{qcr}\selectfont id}, {\fontfamily{qcr}\selectfont text\_native}),
\item a unified four-way fine label ({\fontfamily{qcr}\selectfont Not Toxic}, {\fontfamily{qcr}\selectfont Hard to Say}, {\fontfamily{qcr}\selectfont Toxic}, {\fontfamily{qcr}\selectfont Very Toxic}),
\item a three-way coarse label ({\fontfamily{qcr}\selectfont NonToxic}, {\fontfamily{qcr}\selectfont Ambiguous}, {\fontfamily{qcr}\selectfont Toxic}),
\item and lightweight metadata (category list, original labels, basic confidence).
\end{itemize}
Currently we are mainly working with a Binary Strict view, where we drop rows whose coarse label is {\fontfamily{qcr}\selectfont Ambiguous} and keep only clearly non-toxic vs clearly toxic examples.

\subsection{Paired Native/Rōmaji Views}
From each standardized CSV, we create a paired native/rōmaji view using another Python script. For every example that survives the Binary Strict filter, we:
\begin{itemize}
\item keep the original {\fontfamily{qcr}\selectfont id}, labels, and {\fontfamily{qcr}\selectfont text\_native}
\item add a romanized version of the same text as {\fontfamily{qcr}\selectfont text\_romaji}
\item and save one processed file per source.
\end{itemize}
This produces two separate processed files, one for each dataset. Each row is therefore a matched pair (native vs rōmaji) with the same label, which is exactly what we need to evaluate script robustness.

\subsection{Romanization Pipeline}
We romanize Japanese using pykakasi (v2.2+) \cite{pykakasi2024} with a simple, fixed policy:
\begin{itemize}
\item First normalize text with Unicode NFKC.
\item Use Hepburn-style romanization.
\item Do not insert word separators and do not use capitalization or macrons (long vowels are rendered in plain ASCII, e.g., “ou”, “oo”).
\end{itemize}
This gives us deterministic, ASCII-only rōmaji that roughly matches how Japanese is often typed online, and keeps the romanization behavior consistent across all experiments.

\subsection{Compute Resources}
\begin{itemize}
\item Single GPU instance (e.g., NVIDIA V100, 16 GB) 
\item Python 3.9, PyTorch, Hugging Face Transformers
\end{itemize}

\section{Proposed Approach}
\subsection{Data Preprocessing and Validation}
Our preprocessing has two separate scripted stages per dataset:
\begin{enumerate}
\item \textbf{Standardization}\\
This is explained in detail under Section 5.2 and 5.3
\item \textbf{Binary + Pairing}\\
We then run our pairing script on each standardized CSV. This script:
\begin{itemize}
\item filters out rows with an {\fontfamily{qcr}\selectfont Ambiguous} coarse label (Binary Strict view),
\item generates {\fontfamily{qcr}\selectfont text\_romaji} from {\fontfamily{qcr}\selectfont text\_native} using the romanization pipeline above,
\item and writes the paired native/rōmaji CSVs in the processed data folder.
\end{itemize}
\end{enumerate}
Some light sanity checks (spot-checking rows to make sure the rōmaji looks reasonable, and that IDs and labels are preserved) as a final data validation check.

\subsection{Model Fine-tuning}
\begin{itemize}
\item Train each model on both native Japanese and romanized (rōmaji) text independently to measure script-invariance
\item Implement stratified train/test split (80/20) to maintain toxicity class distribution and ensure reproducibility via fixed random seed
\item Use standard hyperparameters: learning rate 2e-5, batch size 16, 3 epochs, AdamW optimizer, dropout 0.1
\item Save best model checkpoint based on validation accuracy for each configuration (model × script type)
\end{itemize} 

\section{Evaluation}

Our evaluation strategy aims to validate our hypotheses and answer our research questions.

\subsection{Script-Invariance Evaluation Framework}
\label{subsec:script-invariance}

We evaluate all three models (mDeBERTa-v3, BERT Japanese, ByT5) using a unified set of metrics that quantify how well they maintain toxicity detection performance when switching from native Japanese to romaji:

\begin{itemize}
\item \textbf{Per-Script Performance:} Standard classification metrics (Precision, Recall, F1-score per-class and macro-averaged) computed separately for native and romaji test sets
\item \textbf{Script-Invariance Gap:} $\Delta F_1 = F_{1,\text{native}} - F_{1,\text{romaji}}$ with 95\% confidence intervals. Smaller $|\Delta F_1|$ indicates better script robustness.
\item \textbf{Label Consistency Rate:} Percentage of test instances with identical predictions across native and romaji versions (Flip rate = 1 - Consistency)
\item \textbf{Statistical Significance:} McNemar's test on paired native vs romaji predictions to assess whether error rate differences are statistically significant
\item \textbf{Error Pattern Analysis:} Manual review of top 50 high-$\Delta F_1$ cases to identify systematic failure modes
\item \textbf{Efficiency Metrics (Optional):} Inference latency (ms/sample), throughput (samples/s), peak GPU memory
\end{itemize}

These metrics establish a baseline understanding of script-invariance performance for each model, which we then use in the comparative analysis below.

\subsection{Tokenization vs Tokenization-free Model Comparison}
\label{subsec:tokenization-comparison}

Using the metrics from Section~\ref{subsec:script-invariance}, we perform targeted comparative analysis to answer our second research question: does tokenization cause romaji performance degradation, and do byte-level models solve this problem?

\subsubsection{Tokenization Failure Diagnostics}

For tokenized models only (BERT Japanese, mDeBERTa), we measure tokenization-specific artifacts on romaji text:

\begin{itemize}
\item \textbf{Out-of-Vocabulary (OOV) Rate:} Percentage of romaji tokens absent from model vocabulary
\item \textbf{Unknown Token Frequency:} Count of [UNK] tokens per sentence on romaji vs native
\item \textbf{Tokenization Granularity:} Mean tokens/sentence ratio: $\frac{\text{tokens}_{\text{romaji}}}{\text{tokens}_{\text{native}}}$. Values $>1$ indicate fragmentation.
\item \textbf{Segmentation Errors:} For BERT Japanese, qualitative analysis of MeCab failures on romaji (e.g., incorrect morpheme boundaries)
\end{itemize}

\subsubsection{Byte-Level Advantage Analysis}

To verify whether ByT5's byte-level processing provides practical advantages:

\begin{itemize}
\item \textbf{Perturbation Resilience:} Measure $\Delta F_{1,\text{perturbed}} = F_{1,\text{clean}} - F_{1,\text{noisy}}$ on romaji with controlled noise (typos, spacing variations, vowel length ambiguity). Hypothesis: ByT5 shows minimal degradation.
\item \textbf{Romanization Variant Robustness:} Compare error rates across romanization styles (Hepburn vs Kunrei-shiki approximations)
\end{itemize}

\subsubsection{Cross-Architecture Analysis}

\begin{itemize}
\item \textbf{Prediction Agreement:} Cohen's Kappa between tokenized models and ByT5 on romaji. $\kappa < 0.6$ suggests tokenization-induced disagreement; $\kappa > 0.8$ indicates agreement despite different architectures.
\item \textbf{Error Taxonomy:} Categorize romaji test set predictions into four types:
  \begin{itemize}
  \item Type A: All models correct (baseline difficulty)
  \item Type B: Tokenized fail, ByT5 succeeds (tokenization failure evidence)
  \item Type C: ByT5 fails, tokenized succeed (byte-level limitation)
  \item Type D: All fail (inherently difficult)
  \end{itemize}
  High Type B / Type C ratio supports tokenization as primary failure cause.
\item \textbf{Computational Cost-Benefit:} 
  \begin{itemize}
  \item Measure inference latency (median, 95th percentile), GPU memory, throughput for all models
  \item Calculate robustness gain per computational unit: $\frac{|\Delta F_1_{\text{ByT5}}| - |\Delta F_1_{\text{best-tokenized}}|}{\text{latency}_{\text{ByT5}} / \text{latency}_{\text{best-tokenized}}}$
  \item Positive values indicate ByT5's robustness improvement justifies computational overhead
  \end{itemize}
\end{itemize}

This two-stage evaluation design allows us to first establish individual model robustness, then systematically diagnose whether tokenization is the primary factor limiting romaji performance.

\section{Actual Outcomes}

This section summarizes what we have accomplished so far. We've completed the basic implementation and initial testing of our models. While we haven't finished everything we planned, we've made good progress and learned important things about how these models work with Japanese and romaji text.

\subsection{Preliminary Results}
We successfully implemented and evaluated three model architectures on paired native Japanese and romanized toxicity datasets using the llmjp dataset (3847 samples total, with 80/20 train/test split resulting in ~769 test samples).

\subsubsection{Script-Invariance Performance}

Table~\ref{tab:results} shows how each model performs on native Japanese text versus romanized (romaji) text. The key question is whether models can maintain the same toxicity detection accuracy when the only thing that changes is the script—not the meaning. We evaluate this using three metrics: overall accuracy, macro F1 (which treats toxic and non-toxic classes equally), and toxic recall (how well the model catches toxic content).

\begin{table}[h]
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Script} & \textbf{Accuracy} & \textbf{Macro F1} & \textbf{Toxic Recall} & $\Delta F_1$ \\
\midrule
mDeBERTa-v3 & Native & 0.91 & 0.90 & 0.87 & \multirow{2}{*}{-0.13} \\
            & Romaji & 0.77 & 0.77 & 0.90 & \\
\midrule
BERT Japanese & Native & 0.88 & 0.88 & 0.98 & \multirow{2}{*}{-0.12} \\
              & Romaji & 0.78 & 0.76 & 0.61 & \\
\midrule
ByT5-small & Native & 0.61* & 0.54 & 0.27 & \multirow{2}{*}{-0.05} \\
           & Romaji & 0.62* & 0.49 &  0.17& \\
\bottomrule
\multicolumn{6}{l}{\footnotesize *Quick test mode (50 samples, 1 epoch) - full evaluation pending}
\end{tabular}
\caption{Model performance on native Japanese vs romaji toxicity detection}
\label{tab:results}
\end{table}

\subsubsection{Key Findings}
\begin{itemize}
\item \textbf{Tokenized Models Show Script Degradation:} Both mDeBERTa and BERT Japanese experience 12-13 point drops in macro F1 when switching from native to romaji ($\Delta F_1 \approx -0.12$), providing preliminary evidence for H1 (tokenized models show larger performance gaps)
\item \textbf{Different Failure Modes:}
  \begin{itemize}
  \item \textit{mDeBERTa on romaji}: Low non-toxic recall (0.69) but high toxic recall (0.90) - tends to over-predict toxicity
  \item \textit{BERT Japanese on romaji}: High non-toxic recall (0.89) but low toxic recall (0.61) - misses 39\% of toxic content, likely due to MeCab tokenizer failures on romanized text
  \end{itemize}
\item \textbf{ByT5 Requires Further Investigation:} Preliminary results show poor performance on romaji (0.17 accuracy), suggesting training instability or implementation issues that need debugging before we can test H1-H3
\item \textbf{Paired Datasets Established:} Successfully created paired native/romaji datasets from both inspection-ai (310 samples, for quick prototyping) and llmjp (3847 samples, used for reported results)
\end{itemize}

\section{Future Work}

We still have several important tasks to finish before we can fully answer our research questions. This section outlines what we need to do next, organized by the evaluation framework defined in Section~\ref{subsec:script-invariance} and Section~\ref{subsec:tokenization-comparison}.

\subsection{Complete Script-Invariance Evaluation}
\begin{itemize}
\item \textbf{Statistical Significance Testing:} Apply McNemar's test to determine if native vs romaji error rate differences are statistically significant for mDeBERTa and BERT Japanese
\item \textbf{Label Consistency Analysis:} Calculate flip rates and consistency percentages across script changes
\item \textbf{Perturbation Robustness:} Test all models on romanization variants (long-vowel representations, gemination styles, numeric leet)
\item \textbf{Systematic Error Analysis:} Manual review of top 50 high-$\Delta F_1$ cases to identify failure patterns
\end{itemize}

\subsection{Complete Tokenization vs Tokenization-free Comparison}

\subsubsection{Immediate Priority: Fix ByT5 Training}
\begin{itemize}
\item \textbf{Debug ByT5 Implementation:} Investigate the poor romaji performance (0.17 accuracy). Potential issues include:
  \begin{itemize}
  \item Encoder-only usage in current implementation vs full encoder-decoder architecture
  \item Learning rate or batch size unsuitable for byte-level models
  \item Insufficient training epochs (currently 1 epoch in quick test, 3 in full mode)
  \end{itemize}
  Implement proper T5 fine-tuning with decoder inputs or switch to ByT5-base encoder representations
\item \textbf{Full-Scale ByT5 Training:} Train ByT5 on complete dataset with sufficient epochs to enable hypothesis testing (H1-H3)
\end{itemize}

\subsubsection{Hypothesis Testing}
We need to validate the hypothesis on ByT5 model because the current implementation only focuses on the tokenized models.

\subsubsection{Tokenization Diagnostics}
\begin{itemize}
\item \textbf{Vocabulary Analysis:} Measure OOV rates and [UNK] token frequency for BERT Japanese and mDeBERTa on romaji
\item \textbf{Tokenization Granularity:} Calculate tokens/sentence ratios (romaji vs native) to quantify fragmentation
\item \textbf{MeCab Segmentation Analysis:} Identify systematic MeCab failures on romanized text for BERT Japanese
\end{itemize}

\subsubsection{Cross-Architecture Analysis}
\begin{itemize}
\item \textbf{Prediction Agreement:} Calculate Cohen's Kappa between tokenized models and ByT5 on romaji predictions
\item \textbf{Error Taxonomy:} Implement Type A/B/C/D categorization to quantify when tokenization vs byte-level processing fails
\item \textbf{Computational Cost-Benefit:} Measure inference latency, GPU memory, throughput; calculate robustness gain per computational unit
\end{itemize}

\section{Expected Final Outcomes}

This section describes what we hope to achieve by answering our research questions and how our work will contribute to Japanese NLP researhc and practical toxicity detection systems.

\subsection{Research Contributions}
\begin{itemize}
\item \textbf{Script-Invariance Benchmark:} Comprehensive paired native/romaji evaluation for Japanese toxicity detection, demonstrating whether current models maintain fairness across script changes
\item \textbf{Tokenization vs Tokenization-free Comparison:} Quantitative evidence on whether byte-level models (ByT5) outperform subword tokenized models (BERT Japanese, mDeBERTa) on romanized text
\item \textbf{Failure Mode Taxonomy:} Systematic characterization of when and why tokenization breaks down on romaji (MeCab segmentation errors, OOV rates, [UNK] tokens) vs when byte-level processing fails
\end{itemize}

\subsection{Practical Deliverables}
\begin{itemize}
\item \textbf{Reproducible Training Pipeline:} Open-source code supporting multiple model architectures with script selection, enabling researchers to extend this work
\item \textbf{Paired Datasets:} Two-tier benchmark system with native Japanese and Hepburn romaji versions:
  \begin{itemize}
  \item Small-scale (inspection-ai): 310 samples for quick prototyping and testing
  \item Large-scale (llmjp): 3847 samples for production-grade model training
  \end{itemize}
\item \textbf{Model Performance Baselines:} Published results for mDeBERTa-v3, BERT Japanese, and ByT5 on both native and romaji, establishing performance expectations for future work
\end{itemize}

\begin{thebibliography}{9}

\bibitem{mathur2020hateval}
Hammad Rizwan, Muhammad Haroon Shakeel, and Asim Karim. 2020. Hate-Speech and Offensive Language Detection in Roman Urdu.
\textit{In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 2512–2522, Online. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2020.emnlp-main.197}

\bibitem{nature2024multilingual}
Ashiq, W., Kanwal, S., Rafique, A. et al. Roman urdu hate speech detection using hybrid machine learning models and hyperparameter optimization. Sci Rep 14, 28590 (2024).
\url{https://doi.org/10.1038/s41598-024-79106-7}

\bibitem{bertjapanesemodel}
Tohoku NLP. (n.d.). \textit{bert-base-japanese-v3}. Hugging Face.\\
\url{https://huggingface.co/tohoku-nlp/bert-base-japanese-v3}

\bibitem{mdebertav3model}
Microsoft. (n.d.). \textit{mdeberta-v3-base}. Hugging Face.\\
\url{https://huggingface.co/microsoft/mdeberta-v3-base}

\bibitem{googlebyt5model}
Google. (n.d.). \textit {byt5-small}. Hugging Face.\\
\url{https://huggingface.co/google/byt5-small}

\bibitem{llmjp2024toxicity}
LLM-jp Consortium (2024).
\textit{Japanese Toxicity Dataset v2}.
\url{https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-toxicity-dataset-v2}

\bibitem{inspection2024japanese}
Inspection AI (2024).
\textit{Japanese Toxic Dataset}.
\url{https://github.com/inspection-ai/japanese-toxic-dataset}

\bibitem{pykakasi2024}
pykakasi Development Team (2024).
\textit{pykakasi: Japanese text transliteration library}.
\url{https://pypi.org/project/pykakasi}

\bibitem{emnlp2022industry}
Oikawa, Yuto, Yuki Nakayama, and Koji Murakami. 2022. 
\textit{A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat.} 
In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 571–578, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2022.emnlp-industry.58}

\end{thebibliography} 

\end{document}