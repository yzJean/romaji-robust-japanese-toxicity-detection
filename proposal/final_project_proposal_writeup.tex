\documentclass[11pt]{article}

% --- Margins (similar to ACL) ---
\usepackage[a4paper,margin=2.5cm]{geometry}

% --- Fonts ---
\usepackage{times}      % main text font
\usepackage{latexsym}   % symbols for NLP-style papers
\usepackage{url}        % for urls

% --- Japanese language support ---
\usepackage[utf8]{inputenc}
\usepackage{CJKutf8}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage[square,numbers]{natbib}  % For bibliography management with square brackets


% --- Section spacing (ACL style tweaks) ---
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{2ex plus 1ex minus .2ex}{1.5ex plus .2ex}
\titlespacing*{\subsection}{0pt}{1.8ex plus .5ex minus .2ex}{0.8ex plus .2ex}
\titlespacing*{\subsubsection}{0pt}{1.5ex plus .5ex minus .2ex}{0.5ex plus .2ex}

% --- Caption styling ---
\usepackage{caption}
\captionsetup{font=small, labelfont=bf}

\title{Final Project Proposal}
\author{
\begin{tabular}{c c c}
YuChen (Jean) Lin & Pakhi Chatterjee & Aditya Pandey\\
\texttt{lin.4842@osu.edu} & \texttt{chatterjee.197@osu.edu} & \texttt{pandey.172@osu.edu}
\end{tabular}
}
\date{}

\begin{document}

\maketitle

\section{Problem Statement}
Toxic comment detection models for Japanese are typically trained and evaluated on native-script text (kanji/kana). In practice, however, social text often appears in rōmaji (romanized Japanese) for ease of typing, stylistic effect, or to evade filters. Yet most open models are not evaluated for \textit{script invariance}: whether predictions stay stable when only the script changes and the underlying Japanese sentence stays the same. \\
\\
If a model flags \begin{CJK}{UTF8}{min}最低だ\end{CJK} as toxic but misses the semantically identical "saiteida," moderation quality and fairness suffer. Recent Japanese toxicity resources exist, but they focus on native script; a paired, cross-script robustness evaluation is missing. We target that gap by building a lightweight script-invariant toxicity pipeline and a paired testbed that measures label stability when only the script changes. 

\section{Motivation}
Japan has tightened penalties for online insults in 2022, making serious cases punishable by up to one year of imprisonment, so reliable toxicity detection in Japanese is no longer just an academic concern. At the same time, moderation quality is known to be uneven outside English, with evidence of under-investment and patchy language coverage across platforms. Romanized Japanese is ubiquitous because standard IMEs accept rōmaji keystrokes and convert them to kana/kanji; users can also intentionally leave text unconverted. \\
\\
If safety systems only work reliably on native script, they will systematically miss toxic content written in rōmaji and treat some speakers less fairly than others. We therefore treat romanized Japanese as a \textit{first-class input type} and ask whether modern models actually behave robustly under script changes.  

\section{Research Gap}
Recent Japanese toxicity datasets such as LLM-jp's Japanese Toxicity Dataset (v2)\cite{llmjp2024toxicity} provide manually labeled toxic comments in native Japanese to support safer LLMs. Industry work on Japanese toxicity detection emphasizes efficiency and deployment concerns but similarly evaluates almost exclusively on native script. Parallel native/romanized benchmarks for Japanese toxicity are, to our knowledge, missing. \\
\\
Romanized text has been recognized as a real failure mode in other languages. For example, Roman Urdu hate-speech detection work constructs dedicated datasets and models because romanization changes surface forms and challenges tokenizers \cite{mathur2020hateval}. A recent Nature-style line of work on multilingual toxicity \cite{nature2024multilingual} highlights script-agnostic robustness as a key requirement for safe moderation, but no equivalent study has focused on Japanese rōmaji. \\
\\
Separately, modern NLP offers heterogeneous modeling approaches:
\begin{itemize}
\item \textbf{Language-specific subword models} such as Japanese BERT \cite{bertjapanesemodel} are strong native-script baselines widely fine-tuned for Japanese tasks.
\item \textbf{Multilingual subword models} such as mDeBERTa-v3 \cite{mdebertav3model} are pre-trained on the CC100 corpus covering ~100 languages, including Japanese, offering a cross-lingual baseline.
\item \textbf{Tokenizer-free models} such as ByT5 \cite{googlebyt5model} operate directly on raw UTF-8 bytes rather than on subword tokens, and are advertised as more robust to spelling noise and script issues.
\end{itemize}
However, we lack a systematic comparison of these model families on a script-controlled Japanese toxicity benchmark. This project fills that gap by building a paired native/romaji Japanese toxicity dataset and comparing how a language-specific subword model, a multilingual subword model, and a tokenizer-free byte-level model behave under script changes.

\section{Technical Resources and Datasets}
\subsection{Models}
\begin{itemize}
\item BERT base Japanese (Hugging Face) \cite{bertjapanesemodel}
\item Microsoft DeBERTaV3 (Hugging Face) \cite{mdebertav3model}
\item ByT5 - Small (Hugging Face) \cite{googlebyt5model}
\end{itemize}

\subsection{Training Datasets}
\begin{itemize}
\item llm-jp-toxicity-dataset \cite{llmjp2024toxicity}: Japanese toxicity dataset with LLM‐provided quality assessments and statistics 
\item inspection-ai/japanese-toxic-dataset \cite{inspection2024japanese}: Contains native Japanese text with toxicity level annotations
\end{itemize} 

\subsection{Romanization Pipeline}
pykakasi (v2.2+) \cite{pykakasi2024} for reliable Japanese-to-romaji conversion using Hepburn romanization standard 

\subsection{Compute Resources}
\begin{itemize}
\item Single GPU instance (e.g., NVIDIA V100, 16 GB) 
\item Python 3.9, PyTorch, Hugging Face Transformers
\end{itemize}

\section{Proposed Approach}
\subsection{Data Preprocessing and Validation}
\begin{itemize}
\item Convert Japanese datasets to romaji using pykakasi with Hepburn romanization 
\item Manually inspect 200 random samples; record and fix conversion errors (e.g., loanwords, long vowels)
\item Create train/validation/test splits (70/15/15) maintaining toxicity level distribution 
\item Produce summary statistics: token counts, class distributions
\end{itemize}

\subsection{Model Fine-tuning}
\begin{itemize}
\item Fine-tune three model architectures on binary toxicity classification:
  \begin{itemize}
  \item \textbf{BERT base Japanese} (tohoku-nlp/bert-base-japanese-v3): Language-specific subword model with MeCab tokenizer
  \item \textbf{mDeBERTa-v3 base} (microsoft/mdeberta-v3-base): Multilingual subword model pre-trained on 100+ languages
  \item \textbf{ByT5-small} (google/byt5-small): Tokenization-free byte-level encoder-decoder model
  \end{itemize}
\item Train each model on both native Japanese and romanized (rōmaji) text independently to measure script-invariance
\item Implement stratified train/test split (80/20) to maintain toxicity class distribution and ensure reproducibility via fixed random seed
\item Use standard hyperparameters: learning rate 2e-5, batch size 16, 3 epochs, AdamW optimizer, dropout 0.1
\item Save best model checkpoint based on validation accuracy for each configuration (model × script type)
\end{itemize} 

\section{Evaluation}
\subsection{Classification Performance}
\begin{itemize}
\item F1\_native: Precision, Recall, and F1-score on the held-out native-script test set (per level and macro-averaged)
\item F1\_romaji: Precision, Recall, and F1-score on the held-out romaji test set (per level and macro-averaged)
\end{itemize}

\subsection{Script-Invariance Gap}
\begin{itemize}
\item For each toxicity level:
\[
\Delta F_1 = F_{1,\text{native}} - F_{1,\text{r\={o}maji}}
\]
report mean $\Delta$F1 and 95\% confidence interval
\item Label Consistency Rate: \% of test instances whose predicted label is identical in native and romaji forms (Flip rate = 1 - Consistency).
\end{itemize}

\subsection{Statistical Significance}
\begin{itemize}
\item McNemar's Test: Conduct McNemar's test on paired native versus romaji predictions to determine if the difference in error rates is statistically significant
\end{itemize}

\subsection{Robustness to Romanization Variants}
\begin{itemize}
\item For each common perturbation (long-vowel representation, gemination, numeric leet), compute 
\[
\Delta F_{1,\text{variant}} = F_{1,\text{r\={o}maji}} - F_{1,\text{variant}}
\]
report mean and standard deviation
\end{itemize}

\subsection{Error Analysis on Romanized Inputs}
\begin{itemize}
\item High-$\Delta$F1 Case Review: Manually examine the top 50 samples with the largest native–romaji F1 discrepancy to identify systematic romanization failure modes
\end{itemize}

\subsection{Efficiency Metrics for Deployment (Optional)}
\begin{itemize}
\item Inference latency (ms/sample) and throughput (samples/s) on GPU and CPU
\item Peak GPU memory usage during batch inference
\end{itemize}

\subsection{Tokenization vs Tokenization-free Model Comparison}
\label{subsec:tokenization-comparison}

This section addresses our second main research goal: evaluating whether tokenization-free models like ByT5 maintain better toxicity detection performance across scripts compared to tokenized models (BERT Japanese, mDeBERTa). If tokenization is causing the performance drops we observed on romaji, then byte-level models that process raw text without tokenization might be the solution.

\subsubsection{Comparative Performance Metrics}

First, we need to establish whether there's actually a performance difference between tokenized and tokenization-free models on romaji. These metrics directly measure which model type maintains better performance when switching from native Japanese to romaji.

\begin{itemize}
\item \textbf{Script-Invariance Gap:} For each model, compute $\Delta F_1 = F_{1,\text{native}} - F_{1,\text{romaji}}$. Compare:
  \begin{itemize}
  \item Tokenized models (BERT Japanese, mDeBERTa): Expected larger $|\Delta F_1|$ due to tokenization mismatch on romaji
  \item Tokenization-free (ByT5): Expected smaller $|\Delta F_1|$ due to byte-level robustness
  \end{itemize}
\item \textbf{Toxic Recall Degradation:} Measure $\Delta \text{Recall}_{\text{toxic}} = \text{Recall}_{\text{native}} - \text{Recall}_{\text{romaji}}$ to assess which architecture better preserves toxic detection capability across scripts
\item \textbf{Macro-averaged F1 Comparison:} Report macro F1 on romaji test set for all three models to directly compare overall performance on romanized text
\end{itemize}

\subsubsection{Tokenized Model Diagnostic Metrics}

If tokenized models perform poorly on romaji, we need to understand why. This subsection focuses on measuring specific tokenization problems that affect BERT Japanese and mDeBERTa when they process romanized text.

\begin{itemize}
\item \textbf{Out-of-Vocabulary (OOV) Rate:} Percentage of romaji tokens not in the model's vocabulary
\item \textbf{Unknown Token Frequency:} Count of [UNK] tokens per sentence when processing romaji text
\item \textbf{Tokenization Granularity:} Average tokens per sentence (native vs romaji) to quantify fragmentation
\item \textbf{MeCab Fragmentation Examples:} For BERT Japanese, identify cases where MeCab incorrectly segments romaji
\end{itemize}

\subsubsection{Tokenization-free Model Advantage Metrics}

ByT5 should theoretically handle romaji variations better because it processes text at the byte level instead of breaking it into tokens. These metrics test whether ByT5 actually shows this advantage when dealing with spelling variations and noise in romaji.

\begin{itemize}
\item \textbf{Perturbation Resilience:} Introduce controlled noise (typos, spacing variations, vowel length ambiguity) to romaji. Measure F1 degradation: $\Delta F_{1,\text{perturbed}} = F_{1,\text{clean}} - F_{1,\text{noisy}}$. Hypothesis: ByT5 shows minimal degradation
\item \textbf{Character-level Robustness:} Test on romanization variants (e.g., "ou" vs "ō", "kka" vs "kka"). Compare error rates across model types
\end{itemize}

\subsubsection{Cross-Architecture Analysis}

Finally, we need to understand where the models agree and disagree, and whether the computational cost of ByT5 is justified by its performance gains. This subsection analyzes prediction patterns and cost-benefit trade-offs.

\begin{itemize}
\item \textbf{Prediction Agreement:} Calculate Cohen's Kappa between each tokenized model and ByT5 on romaji predictions. Kappa values: $\kappa < 0.6$ indicates moderate or lower agreement, suggesting tokenization-induced prediction differences; $\kappa > 0.8$ indicates substantial agreement, suggesting tokenization is not the primary factor affecting predictions
\item \textbf{Error Type Distribution and Pattern Analysis:} For romaji test set, categorize errors into four types:
  \begin{itemize}
  \item Type A: Both tokenized and tokenization-free models correct
  \item Type B: Tokenized models fail, ByT5 succeeds (tokenization failure)
  \item Type C: ByT5 fails, tokenized models succeed (byte-level limitation)
  \item Type D: Both fail (inherently difficult samples)
  \end{itemize}
  Report percentage distribution and analyze Type B vs Type C ratio. For Type B and Type C cases, conduct qualitative analysis to identify systematic patterns (e.g., linguistic features, token characteristics, sequence length) that distinguish when tokenization vs byte-level processing fails
\item \textbf{Computational Cost-Benefit Analysis:}
  \begin{itemize}
  \item Inference latency (ms): median and 95th percentile across 1000 samples
  \item GPU memory usage (MB): peak allocation during batch inference (batch size = 16)
  \item Throughput (samples/second): processing speed on full test set
  \item Robustness gain per computational unit: $\frac{\Delta F_1 \text{ improvement}}{\text{inference time ratio}}$ comparing ByT5 to fastest tokenized model. Higher value indicates better efficiency trade-off (more robustness improvement per unit of computational cost); lower value indicates worse trade-off (expensive computational cost for small gains)
  \end{itemize}
\end{itemize}

\section{Actual Outcomes}

This section summarizes what we have accomplished so far. We've completed the basic implementation and initial testing of our models. While we haven't finished everything we planned, we've made good progress and learned important things about how these models work with Japanese and romaji text.

\subsection{Preliminary Results}
We successfully implemented and evaluated three model architectures on paired native Japanese and romanized toxicity datasets using the llmjp dataset (955K samples total, with 80/20 train/test split resulting in ~191K test samples).

\subsubsection{Script-Invariance Performance}

Table~\ref{tab:results} shows how each model performs on native Japanese text versus romanized (romaji) text. The key question is whether models can maintain the same toxicity detection accuracy when the only thing that changes is the script—not the meaning. We evaluate this using three metrics: overall accuracy, macro F1 (which treats toxic and non-toxic classes equally), and toxic recall (how well the model catches toxic content).

\begin{table}[h]
\centering
\small
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Script} & \textbf{Accuracy} & \textbf{Macro F1} & \textbf{Toxic Recall} & $\Delta F_1$ \\
\midrule
mDeBERTa-v3 & Native & 0.91 & 0.90 & 0.87 & \multirow{2}{*}{-0.13} \\
            & Romaji & 0.77 & 0.77 & 0.90 & \\
\midrule
BERT Japanese & Native & 0.88 & 0.88 & 0.98 & \multirow{2}{*}{-0.12} \\
              & Romaji & 0.78 & 0.76 & 0.61 & \\
\midrule
ByT5-small & Native & 0.83* & - & - & \multirow{2}{*}{TBD} \\
           & Romaji & 0.17* & - & - & \\
\bottomrule
\multicolumn{6}{l}{\footnotesize *Quick test mode (50 samples, 1 epoch) - full evaluation pending}
\end{tabular}
\caption{Model performance on native Japanese vs romaji toxicity detection}
\label{tab:results}
\end{table}

\subsubsection{Key Findings}
\begin{itemize}
\item \textbf{Tokenized Models Show Script Degradation:} Both mDeBERTa and BERT Japanese experience 12-13 point drops in macro F1 when switching from native to romaji ($\Delta F_1 \approx -0.12$), confirming our hypothesis about tokenization-induced brittleness
\item \textbf{Different Failure Modes:}
  \begin{itemize}
  \item \textit{mDeBERTa on romaji}: Low non-toxic recall (0.69) but high toxic recall (0.90) - tends to over-predict toxicity
  \item \textit{BERT Japanese on romaji}: High non-toxic recall (0.89) but low toxic recall (0.61) - misses 39\% of toxic content, likely due to MeCab tokenizer failures on romanized text
  \end{itemize}
\item \textbf{ByT5 Requires Further Investigation:} Preliminary results show poor performance on romaji (0.17 accuracy), suggesting training instability or implementation issues that need debugging
\item \textbf{Paired Datasets Established:} Successfully created paired native/romaji datasets from both inspection-ai (310 samples, for quick prototyping) and llmjp (955K samples, used for reported results)
\end{itemize}

\subsection{Deliverables Completed}
\begin{itemize}
\item Paired native-romaji Japanese toxicity benchmarks:
  \begin{itemize}
  \item Small-scale: inspection-ai dataset (310 samples, binary classification)
  \item Large-scale: llmjp dataset (955K samples, binary classification)
  \end{itemize}
\item Training pipeline supporting three model architectures with script selection (--use-romaji flag)
\item Reproducible training scripts with fixed random seeds and stratified splits
\item Comprehensive evaluation metrics: accuracy, precision, recall, F1 (per-class and macro-averaged), confusion matrices
\item Model checkpoints and result files for mDeBERTa and BERT Japanese on both scripts
\end{itemize}

\section{Future Work}

We still have several important tasks to finish before we can fully answer our research questions. This section outlines what we need to do next, organized by priority. The main challenge is getting ByT5 to work properly so we can compare it fairly with the other models.

\subsection{Immediate Priorities}
\begin{itemize}
\item \textbf{Debug ByT5 Training:} Investigate the poor romaji performance (0.17 accuracy). Potential issues include:
  \begin{itemize}
  \item Encoder-only usage in current implementation vs full encoder-decoder architecture
  \item Learning rate or batch size unsuitable for byte-level models
  \item Insufficient training epochs (currently 1 epoch in quick test, 3 in full mode)
  \end{itemize}
  Implement proper T5 fine-tuning with decoder inputs or switch to ByT5-base encoder representations
\item \textbf{Full-Scale ByT5 Training:} Train ByT5 on complete dataset with sufficient epochs to enable fair comparison with tokenized models
\item \textbf{Compute Comprehensive Metrics:} Calculate all proposed evaluation metrics from Section~\ref{subsec:tokenization-comparison}, including OOV rates, tokenization granularity, and Cohen's Kappa
\end{itemize}

\subsection{Advanced Analysis}
\begin{itemize}
\item \textbf{Error Type Distribution:} Implement the 4-type taxonomy (Type A/B/C/D) to quantify when tokenized vs tokenization-free models fail
\item \textbf{Tokenization Artifact Analysis:} 
  \begin{itemize}
  \item Measure [UNK] token frequency for BERT Japanese on romaji
  \item Analyze MeCab segmentation failures on romanized text
  \item Compare tokenization granularity (tokens/sentence) across native vs romaji for each model
  \end{itemize}
\end{itemize}

\subsection{Efficiency and Deployment Metrics}
\begin{itemize}
\item \textbf{Computational Cost-Benefit Analysis:} Measure inference latency, GPU memory usage, and throughput for all three models
\item \textbf{Calculate Robustness Gain per Computational Unit:} Quantify whether ByT5's potential robustness improvements justify computational overhead
\end{itemize}

\section{Expected Final Outcomes}

Once we complete all the tasks listed in the Future Work section, we expect to have clear answers to our research questions. This section describes what we hope to achieve and how our work will contribute to Japanese NLP research and practical toxicity detection systems.

\subsection{Research Contributions}
\begin{itemize}
\item \textbf{Script-Invariance Benchmark:} First comprehensive paired native/romaji evaluation for Japanese toxicity detection, demonstrating whether current models maintain fairness across script changes
\item \textbf{Tokenization vs Tokenization-free Comparison:} Quantitative evidence on whether byte-level models (ByT5) outperform subword tokenized models (BERT Japanese, mDeBERTa) on romanized text
\item \textbf{Failure Mode Taxonomy:} Systematic characterization of when and why tokenization breaks down on romaji (MeCab segmentation errors, OOV rates, [UNK] tokens) vs when byte-level processing fails
\end{itemize}

\subsection{Practical Deliverables}
\begin{itemize}
\item \textbf{Reproducible Training Pipeline:} Open-source code supporting multiple model architectures with script selection, enabling researchers to extend this work
\item \textbf{Paired Datasets:} Two-tier benchmark system with native Japanese and Hepburn romaji versions:
  \begin{itemize}
  \item Small-scale (inspection-ai): 310 samples for quick prototyping and testing
  \item Large-scale (llmjp): 955K samples for production-grade model training
  \end{itemize}
\item \textbf{Model Performance Baselines:} Published results for mDeBERTa-v3, BERT Japanese, and ByT5 on both native and romaji, establishing performance expectations for future work
\end{itemize}

\subsection{Key Insights}
\begin{itemize}
\item \textbf{Efficiency-Robustness Trade-offs:} Quantified computational cost (inference latency, memory) vs script-invariance gains, informing deployment decisions
\item \textbf{Architectural Recommendations:} Evidence-based guidance on when to use tokenization-free models for Japanese (and by extension, other languages with romanization)
\item \textbf{Generalization Potential:} Insights applicable to Korean, Arabic, Hindi, and other languages where romanization creates script-invariance challenges
\end{itemize}

\subsection{Impact on Japanese NLP}
\begin{itemize}
\item \textbf{Moderation Fairness:} Demonstrate whether existing models systematically disadvantage users who write in romaji, highlighting gaps in current content moderation systems
\item \textbf{Model Selection Criteria:} Provide practitioners with quantitative metrics to choose between models based on deployment context (native-only vs mixed-script environments)
\item \textbf{Future Research Directions:} Identify remaining challenges (e.g., ByT5 training stability, perturbation robustness) to guide next-generation toxicity detection research
\end{itemize} 

\begin{thebibliography}{9}

\bibitem{mathur2020hateval}
Hammad Rizwan, Muhammad Haroon Shakeel, and Asim Karim. 2020. Hate-Speech and Offensive Language Detection in Roman Urdu.
\textit{In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 2512–2522, Online. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2020.emnlp-main.197}

\bibitem{nature2024multilingual}
Ashiq, W., Kanwal, S., Rafique, A. et al. Roman urdu hate speech detection using hybrid machine learning models and hyperparameter optimization. Sci Rep 14, 28590 (2024).
\url{https://doi.org/10.1038/s41598-024-79106-7}

\bibitem{bertjapanesemodel}
Tohoku NLP. (n.d.). \textit{bert-base-japanese-v3}. Hugging Face.\\
\url{https://huggingface.co/tohoku-nlp/bert-base-japanese-v3}

\bibitem{mdebertav3model}
Microsoft. (n.d.). \textit{mdeberta-v3-base}. Hugging Face.\\
\url{https://huggingface.co/microsoft/mdeberta-v3-base}

\bibitem{googlebyt5model}
Google. (n.d.). \textit {byt5-small}. Hugging Face.\\
\url{https://huggingface.co/google/byt5-small}

\bibitem{llmjp2024toxicity}
LLM-jp Consortium (2024).
\textit{Japanese Toxicity Dataset v2}.
\url{https://gitlab.llm-jp.nii.ac.jp/datasets/llm-jp-toxicity-dataset-v2}

\bibitem{inspection2024japanese}
Inspection AI (2024).
\textit{Japanese Toxic Dataset}.
\url{https://github.com/inspection-ai/japanese-toxic-dataset}

\bibitem{pykakasi2024}
pykakasi Development Team (2024).
\textit{pykakasi: Japanese text transliteration library}.
\url{https://pypi.org/project/pykakasi}

\bibitem{emnlp2022industry}
Oikawa, Yuto, Yuki Nakayama, and Koji Murakami. 2022. 
\textit{A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat.} 
In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 571–578, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2022.emnlp-industry.58}

\end{thebibliography} 

\end{document}